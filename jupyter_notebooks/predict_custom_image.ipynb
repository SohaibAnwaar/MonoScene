{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is created to run this repo on a single gpu machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dac2399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding Base Dir Path /monoscene/MonoScene\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "BASE_DIR = os.getcwd().rsplit(\"/\",1)[0]\n",
    "sys.path.append(BASE_DIR)\n",
    "print(\"adding Base Dir Path\", BASE_DIR)\n",
    "\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import numpy.matlib\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from monoscene.data.utils.helpers import (\n",
    "    vox2pix,\n",
    "    compute_local_frustums,\n",
    "    compute_CP_mega_matrix,\n",
    ")\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "from monoscene.models.monoscene import MonoScene\n",
    "from monoscene.data.NYU.nyu_dm import NYUDataModule, collate_fn, NYUDataset\n",
    "from monoscene.data.semantic_kitti.kitti_dm import KittiDataModule\n",
    "from monoscene.data.kitti_360.kitti_360_dm import Kitti360DataModule\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from hydra.utils import get_original_cwd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3db5f6",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbd68150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from types import SimpleNamespace\n",
    "\n",
    "with open(\"../monoscene/config/monoscene.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "        config = SimpleNamespace(**config)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b0dd9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(dataset='kitti_360',\n",
       "          n_relations=4,\n",
       "          enable_log=False,\n",
       "          kitti_root='/path/to/semantic_kitti',\n",
       "          kitti_preprocess_root='/path/to/kitti/preprocess/folder',\n",
       "          kitti_logdir='/path/to/semantic_kitti/logdir',\n",
       "          NYU_root='/monoscene/MonoScene/nyu/NYU_dataset/depthbin/',\n",
       "          NYU_preprocess_root='/monoscene/MonoScene/nyu/NYU_dataset/depthbin/preprocess',\n",
       "          logdir='/monoscene/logdir',\n",
       "          output_path='/path/to/output',\n",
       "          fp_loss=True,\n",
       "          frustum_size=8,\n",
       "          batch_size=1,\n",
       "          n_gpus=0,\n",
       "          num_workers_per_gpu=0,\n",
       "          exp_prefix='exp',\n",
       "          run=1,\n",
       "          lr='1e-4',\n",
       "          weight_decay='1e-4',\n",
       "          context_prior=True,\n",
       "          relation_loss=True,\n",
       "          CE_ssc_loss=True,\n",
       "          sem_scal_loss=True,\n",
       "          geo_scal_loss=True,\n",
       "          project_1_2=True,\n",
       "          project_1_4=True,\n",
       "          project_1_8=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3a73d",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d65580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "seg_class_map = [\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    11,\n",
    "    5,\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    8,\n",
    "    10,\n",
    "    10,\n",
    "    10,\n",
    "    11,\n",
    "    11,\n",
    "    9,\n",
    "    8,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    10,\n",
    "    10,\n",
    "    11,\n",
    "    8,\n",
    "    10,\n",
    "    11,\n",
    "    9,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "]\n",
    "\n",
    "\n",
    "def _rle2voxel(rle, voxel_size=(240, 144, 240), rle_filename=\"\"):\n",
    "    r\"\"\"Read voxel label data from file (RLE compression), and convert it to fully occupancy labeled voxels.\n",
    "    code taken from https://github.com/waterljwant/SSC/blob/master/dataloaders/dataloader.py#L172\n",
    "    In the data loader of pytorch, only single thread is allowed.\n",
    "    For multi-threads version and more details, see 'readRLE.py'.\n",
    "    output: seg_label: 3D numpy array, size 240 x 144 x 240\n",
    "    \"\"\"\n",
    "    seg_label = np.zeros(\n",
    "        int(voxel_size[0] * voxel_size[1] * voxel_size[2]), dtype=np.uint8\n",
    "    )  # segmentation label\n",
    "    vox_idx = 0\n",
    "    for idx in range(int(rle.shape[0] / 2)):\n",
    "        check_val = rle[idx * 2]\n",
    "        check_iter = rle[idx * 2 + 1]\n",
    "        if check_val >= 37 and check_val != 255:  # 37 classes to 12 classes\n",
    "            print(\"RLE {} check_val: {}\".format(rle_filename, check_val))\n",
    "        seg_label_val = (\n",
    "            seg_class_map[check_val] if check_val != 255 else 255\n",
    "        )  # 37 classes to 12 classes\n",
    "        seg_label[vox_idx : vox_idx + check_iter] = np.matlib.repmat(\n",
    "            seg_label_val, 1, check_iter\n",
    "        )\n",
    "        vox_idx = vox_idx + check_iter\n",
    "    seg_label = seg_label.reshape(voxel_size)  # 3D array, size 240 x 144 x 240\n",
    "    return seg_label\n",
    "\n",
    "\n",
    "def _read_rle(rle_filename):  # 0.0005s\n",
    "    \"\"\"Read RLE compression data\n",
    "    code taken from https://github.com/waterljwant/SSC/blob/master/dataloaders/dataloader.py#L153\n",
    "    Return:\n",
    "        vox_origin,\n",
    "        cam_pose,\n",
    "        vox_rle, voxel label data from file\n",
    "    Shape:\n",
    "        vox_rle, (240, 144, 240)\n",
    "    \"\"\"\n",
    "    fid = open(rle_filename, \"rb\")\n",
    "    vox_origin = np.fromfile(\n",
    "        fid, np.float32, 3\n",
    "    ).T  # Read voxel origin in world coordinates\n",
    "    cam_pose = np.fromfile(fid, np.float32, 16).reshape((4, 4))  # Read camera pose\n",
    "    vox_rle = (\n",
    "        np.fromfile(fid, np.uint32).reshape((-1, 1)).T\n",
    "    )  # Read voxel label data from file\n",
    "    vox_rle = np.squeeze(vox_rle)  # 2d array: (1 x N), to 1d array: (N , )\n",
    "    fid.close()\n",
    "    return vox_origin, cam_pose, vox_rle\n",
    "\n",
    "\n",
    "def _downsample_label(label, voxel_size=(240, 144, 240), downscale=4):\n",
    "    r\"\"\"downsample the labeled data,\n",
    "    code taken from https://github.com/waterljwant/SSC/blob/master/dataloaders/dataloader.py#L262\n",
    "    Shape:\n",
    "        label, (240, 144, 240)\n",
    "        label_downscale, if downsample==4, then (60, 36, 60)\n",
    "    \"\"\"\n",
    "    if downscale == 1:\n",
    "        return label\n",
    "    ds = downscale\n",
    "    small_size = (\n",
    "        voxel_size[0] // ds,\n",
    "        voxel_size[1] // ds,\n",
    "        voxel_size[2] // ds,\n",
    "    )  # small size\n",
    "    label_downscale = np.zeros(small_size, dtype=np.uint8)\n",
    "    empty_t = 0.95 * ds * ds * ds  # threshold\n",
    "    s01 = small_size[0] * small_size[1]\n",
    "    label_i = np.zeros((ds, ds, ds), dtype=np.int32)\n",
    "\n",
    "    for i in range(small_size[0] * small_size[1] * small_size[2]):\n",
    "        z = int(i / s01)\n",
    "        y = int((i - z * s01) / small_size[0])\n",
    "        x = int(i - z * s01 - y * small_size[0])\n",
    "\n",
    "        label_i[:, :, :] = label[\n",
    "            x * ds : (x + 1) * ds, y * ds : (y + 1) * ds, z * ds : (z + 1) * ds\n",
    "        ]\n",
    "        label_bin = label_i.flatten()\n",
    "\n",
    "        zero_count_0 = np.array(np.where(label_bin == 0)).size\n",
    "        zero_count_255 = np.array(np.where(label_bin == 255)).size\n",
    "\n",
    "        zero_count = zero_count_0 + zero_count_255\n",
    "        if zero_count > empty_t:\n",
    "            label_downscale[x, y, z] = 0 if zero_count_0 > zero_count_255 else 255\n",
    "        else:\n",
    "            label_i_s = label_bin[\n",
    "                np.where(np.logical_and(label_bin > 0, label_bin < 255))\n",
    "            ]\n",
    "            label_downscale[x, y, z] = np.argmax(np.bincount(label_i_s))\n",
    "    return label_downscale\n",
    "\n",
    "\n",
    "def read_rle_binary(scan: str):\n",
    "    scene_size = (240, 144, 240)\n",
    "\n",
    "    \n",
    "    name = scan[:-4]\n",
    "\n",
    "    vox_origin, cam_pose, rle = _read_rle(scan)\n",
    "\n",
    "    target_1_1 = _rle2voxel(rle, scene_size, scan)\n",
    "    target_1_4 = _downsample_label(target_1_1, scene_size, 4)\n",
    "    target_1_16 = _downsample_label(target_1_1, scene_size, 16)\n",
    "\n",
    "    data = {\n",
    "        \"cam_pose\": cam_pose,\n",
    "        \"voxel_origin\": vox_origin,\n",
    "        \"name\": name,\n",
    "        \"target_1_4\": target_1_4,\n",
    "        \"target_1_16\": target_1_16,\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "454bac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/monoscene/MonoScene/nyu/NYU_dataset/depthbin/NYUtest/NYU0001_0000_color.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_image(data, n_relations=4, color_jitter=None, frustum_size=4, fliplr=0.0,\n",
    "                    voxel_size = 0.08, scene_size = (4.8, 4.8, 2.88), img_W = 640, img_H = 480,\n",
    "                    cam_k = np.array([[518.8579, 0, 320], [0, 518.8579, 240], [0, 0, 1]]),\n",
    "                    n_classes = 12, color_image_path=\"\"):\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    normalize_rgb = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    cam_pose = np.array([[518.8579, 0, 320], [0, 518.8579, 240], [0, 0, 1]])\n",
    "    \n",
    "    cam_pose = data[\"cam_pose\"]\n",
    "    T_world_2_cam = np.linalg.inv(cam_pose)\n",
    "    vox_origin = data[\"voxel_origin\"]\n",
    "    data[\"cam_k\"] = cam_k\n",
    "    target = data[\n",
    "        \"target_1_4\"\n",
    "    ]  # Following SSC literature, the output resolution on NYUv2 is set to 1:4\n",
    "    data[\"target\"] = target\n",
    "    target_1_4 = data[\"target_1_16\"]\n",
    "\n",
    "    CP_mega_matrix = compute_CP_mega_matrix(\n",
    "        target_1_4, is_binary=n_relations == 2\n",
    "    )\n",
    "    data[\"CP_mega_matrix\"] = CP_mega_matrix\n",
    "\n",
    "    # compute the 3D-2D mapping\n",
    "    projected_pix, fov_mask, pix_z = vox2pix(\n",
    "        T_world_2_cam,\n",
    "        cam_k,\n",
    "        vox_origin,\n",
    "        voxel_size,\n",
    "        img_W,\n",
    "        img_H,\n",
    "        scene_size,\n",
    "    )\n",
    "\n",
    "    data[\"projected_pix_1\"] = projected_pix\n",
    "    data[\"fov_mask_1\"] = fov_mask\n",
    "\n",
    "    # compute the masks, each indicates voxels inside a frustum\n",
    "    frustums_masks, frustums_class_dists = compute_local_frustums(\n",
    "        projected_pix,\n",
    "        pix_z,\n",
    "        target,\n",
    "        img_W,\n",
    "        img_H,\n",
    "        dataset=\"NYU\",\n",
    "        n_classes=12,\n",
    "        size=frustum_size,\n",
    "    )\n",
    "    data[\"frustums_masks\"] = frustums_masks\n",
    "    data[\"frustums_class_dists\"] = frustums_class_dists\n",
    "\n",
    "    # Image augmentation\n",
    "    color_jitter = (transforms.ColorJitter(*color_jitter) if color_jitter else None)\n",
    "    if color_jitter is not None:\n",
    "        img = color_jitter(img)\n",
    "\n",
    "    # PIL to numpy\n",
    "    print(color_image_path)\n",
    "    img = Image.open(color_image_path).convert(\"RGB\")\n",
    "    if not color_image_path: raise ValidationError(\"Color Image not Provided\")\n",
    "    img = np.array(img, dtype=np.float32, copy=False) / 255.0\n",
    "\n",
    "    # randomly fliplr the image\n",
    "    if np.random.rand() < fliplr:\n",
    "        img = np.ascontiguousarray(np.fliplr(img))\n",
    "        data[\"projected_pix_1\"][:, 0] = (\n",
    "            img.shape[1] - 1 - data[\"projected_pix_1\"][:, 0]\n",
    "        )\n",
    "\n",
    "    data[\"img\"] = normalize_rgb(img)  # (3, img_H, img_W)\n",
    "\n",
    "    return data\n",
    "\n",
    "color_image_path = \"/monoscene/MonoScene/nyu/NYU_dataset/depthbin/NYUtest/NYU0001_0000_color.jpg\"\n",
    "binary_file = \"/monoscene/MonoScene/nyu/NYU_dataset/depthbin/NYUtest/NYU0001_0000.bin\"\n",
    "rle_data = read_rle_binary(binary_file)\n",
    "data = preprocess_image(rle_data, color_image_path = color_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f44dab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6464c711",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2593b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "project_scale = 1\n",
    "feature = 200\n",
    "full_scene_size = (60, 36, 60)\n",
    "\n",
    "# test_ds = NYUDataset(\n",
    "#             split=\"test\",\n",
    "#             preprocess_root=config.NYU_preprocess_root,\n",
    "#             n_relations=4,\n",
    "#             root=config.NYU_root,\n",
    "#             frustum_size=4,\n",
    "#             fliplr=0.0,\n",
    "#             color_jitter=None,\n",
    "#         )\n",
    "\n",
    "data_loader = DataLoader(\n",
    "            [data],\n",
    "            batch_size=1,\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers=11,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be44c308",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c872262",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/monoscene/MonoScene/trained_models/monoscene_nyu.ckpt\"\n",
    "# model = MonoScene.load_from_checkpoint(\n",
    "#         model_path,\n",
    "#         dataset=\"NYU\",\n",
    "#         feature=feature,\n",
    "#         project_scale=project_scale,\n",
    "#         fp_loss=config.fp_loss,\n",
    "#         full_scene_size=full_scene_size,\n",
    "#     )\n",
    "# model.cuda()\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a750e02",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d7b96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = '/monoscene/Monoscene/outputs/'\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(data_loader):\n",
    "#         batch[\"img\"] = batch[\"img\"].cuda()\n",
    "#         pred = model(batch)\n",
    "#         y_pred = torch.softmax(pred[\"ssc_logit\"], dim=1).detach().cpu().numpy()\n",
    "#         y_pred = np.argmax(y_pred, axis=1)\n",
    "#         for i in range(config.batch_size):\n",
    "#             out_dict = {\"y_pred\": y_pred[i].astype(np.uint16)}\n",
    "#             if \"target\" in batch:\n",
    "#                 out_dict[\"target\"] = (\n",
    "#                     batch[\"target\"][i].detach().cpu().numpy().astype(np.uint16)\n",
    "#                 )\n",
    "            \n",
    "#             # NYU write Path\n",
    "#             write_path = output_path\n",
    "#             filepath = os.path.join(write_path, batch[\"name\"][i] + \".pkl\")\n",
    "#             out_dict[\"cam_pose\"] = batch[\"cam_pose\"][i].detach().cpu().numpy()\n",
    "#             out_dict[\"vox_origin\"] = (\n",
    "#                 batch[\"vox_origin\"][i].detach().cpu().numpy()\n",
    "#             )\n",
    "            \n",
    "#             os.makedirs(write_path, exist_ok=True)\n",
    "#             with open(filepath, \"wb\") as handle:\n",
    "#                 pickle.dump(out_dict, handle)\n",
    "#                 print(\"wrote to\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e061d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Most of the code is taken from https://github.com/andyzeng/tsdf-fusion-python/blob/master/fusion.py\n",
    "@inproceedings{zeng20163dmatch,\n",
    "    title={3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions},\n",
    "    author={Zeng, Andy and Song, Shuran and Nie{\\ss}ner, Matthias and Fisher, Matthew and Xiao, Jianxiong and Funkhouser, Thomas},\n",
    "    booktitle={CVPR},\n",
    "    year={2017}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from numba import njit, prange\n",
    "from skimage import measure\n",
    "\n",
    "FUSION_GPU_MODE = 1\n",
    "\n",
    "\n",
    "class TSDFVolume:\n",
    "    \"\"\"Volumetric TSDF Fusion of RGB-D Images.\"\"\"\n",
    "\n",
    "    def __init__(self, vol_bnds, voxel_size, use_gpu=True):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "          vol_bnds (ndarray): An ndarray of shape (3, 2). Specifies the\n",
    "            xyz bounds (min/max) in meters.\n",
    "          voxel_size (float): The volume discretization in meters.\n",
    "        \"\"\"\n",
    "        vol_bnds = np.asarray(vol_bnds)\n",
    "        assert vol_bnds.shape == (3, 2), \"[!] `vol_bnds` should be of shape (3, 2).\"\n",
    "\n",
    "        # Define voxel volume parameters\n",
    "        self._vol_bnds = vol_bnds\n",
    "        self._voxel_size = float(voxel_size)\n",
    "        self._trunc_margin = 5 * self._voxel_size  # truncation on SDF\n",
    "        # self._trunc_margin = 10  # truncation on SDF\n",
    "        self._color_const = 256 * 256\n",
    "\n",
    "        # Adjust volume bounds and ensure C-order contiguous\n",
    "        self._vol_dim = (\n",
    "            np.ceil((self._vol_bnds[:, 1] - self._vol_bnds[:, 0]) / self._voxel_size)\n",
    "            .copy(order=\"C\")\n",
    "            .astype(int)\n",
    "        )\n",
    "        self._vol_bnds[:, 1] = self._vol_bnds[:, 0] + self._vol_dim * self._voxel_size\n",
    "        self._vol_origin = self._vol_bnds[:, 0].copy(order=\"C\").astype(np.float32)\n",
    "\n",
    "        print(\n",
    "            \"Voxel volume size: {} x {} x {} - # points: {:,}\".format(\n",
    "                self._vol_dim[0],\n",
    "                self._vol_dim[1],\n",
    "                self._vol_dim[2],\n",
    "                self._vol_dim[0] * self._vol_dim[1] * self._vol_dim[2],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Initialize pointers to voxel volume in CPU memory\n",
    "        self._tsdf_vol_cpu = np.zeros(self._vol_dim).astype(np.float32)\n",
    "        # for computing the cumulative moving average of observations per voxel\n",
    "        self._weight_vol_cpu = np.zeros(self._vol_dim).astype(np.float32)\n",
    "        self._color_vol_cpu = np.zeros(self._vol_dim).astype(np.float32)\n",
    "\n",
    "        self.gpu_mode = use_gpu and FUSION_GPU_MODE\n",
    "\n",
    "        # Copy voxel volumes to GPU\n",
    "        if self.gpu_mode:\n",
    "            self._tsdf_vol_gpu = cuda.mem_alloc(self._tsdf_vol_cpu.nbytes)\n",
    "            cuda.memcpy_htod(self._tsdf_vol_gpu, self._tsdf_vol_cpu)\n",
    "            self._weight_vol_gpu = cuda.mem_alloc(self._weight_vol_cpu.nbytes)\n",
    "            cuda.memcpy_htod(self._weight_vol_gpu, self._weight_vol_cpu)\n",
    "            self._color_vol_gpu = cuda.mem_alloc(self._color_vol_cpu.nbytes)\n",
    "            cuda.memcpy_htod(self._color_vol_gpu, self._color_vol_cpu)\n",
    "\n",
    "            # Cuda kernel function (C++)\n",
    "            self._cuda_src_mod = SourceModule(\n",
    "                \"\"\"\n",
    "        __global__ void integrate(float * tsdf_vol,\n",
    "                                  float * weight_vol,\n",
    "                                  float * color_vol,\n",
    "                                  float * vol_dim,\n",
    "                                  float * vol_origin,\n",
    "                                  float * cam_intr,\n",
    "                                  float * cam_pose,\n",
    "                                  float * other_params,\n",
    "                                  float * color_im,\n",
    "                                  float * depth_im) {\n",
    "          // Get voxel index\n",
    "          int gpu_loop_idx = (int) other_params[0];\n",
    "          int max_threads_per_block = blockDim.x;\n",
    "          int block_idx = blockIdx.z*gridDim.y*gridDim.x+blockIdx.y*gridDim.x+blockIdx.x;\n",
    "          int voxel_idx = gpu_loop_idx*gridDim.x*gridDim.y*gridDim.z*max_threads_per_block+block_idx*max_threads_per_block+threadIdx.x;\n",
    "          int vol_dim_x = (int) vol_dim[0];\n",
    "          int vol_dim_y = (int) vol_dim[1];\n",
    "          int vol_dim_z = (int) vol_dim[2];\n",
    "          if (voxel_idx > vol_dim_x*vol_dim_y*vol_dim_z)\n",
    "              return;\n",
    "          // Get voxel grid coordinates (note: be careful when casting)\n",
    "          float voxel_x = floorf(((float)voxel_idx)/((float)(vol_dim_y*vol_dim_z)));\n",
    "          float voxel_y = floorf(((float)(voxel_idx-((int)voxel_x)*vol_dim_y*vol_dim_z))/((float)vol_dim_z));\n",
    "          float voxel_z = (float)(voxel_idx-((int)voxel_x)*vol_dim_y*vol_dim_z-((int)voxel_y)*vol_dim_z);\n",
    "          // Voxel grid coordinates to world coordinates\n",
    "          float voxel_size = other_params[1];\n",
    "          float pt_x = vol_origin[0]+voxel_x*voxel_size;\n",
    "          float pt_y = vol_origin[1]+voxel_y*voxel_size;\n",
    "          float pt_z = vol_origin[2]+voxel_z*voxel_size;\n",
    "          // World coordinates to camera coordinates\n",
    "          float tmp_pt_x = pt_x-cam_pose[0*4+3];\n",
    "          float tmp_pt_y = pt_y-cam_pose[1*4+3];\n",
    "          float tmp_pt_z = pt_z-cam_pose[2*4+3];\n",
    "          float cam_pt_x = cam_pose[0*4+0]*tmp_pt_x+cam_pose[1*4+0]*tmp_pt_y+cam_pose[2*4+0]*tmp_pt_z;\n",
    "          float cam_pt_y = cam_pose[0*4+1]*tmp_pt_x+cam_pose[1*4+1]*tmp_pt_y+cam_pose[2*4+1]*tmp_pt_z;\n",
    "          float cam_pt_z = cam_pose[0*4+2]*tmp_pt_x+cam_pose[1*4+2]*tmp_pt_y+cam_pose[2*4+2]*tmp_pt_z;\n",
    "          // Camera coordinates to image pixels\n",
    "          int pixel_x = (int) roundf(cam_intr[0*3+0]*(cam_pt_x/cam_pt_z)+cam_intr[0*3+2]);\n",
    "          int pixel_y = (int) roundf(cam_intr[1*3+1]*(cam_pt_y/cam_pt_z)+cam_intr[1*3+2]);\n",
    "          // Skip if outside view frustum\n",
    "          int im_h = (int) other_params[2];\n",
    "          int im_w = (int) other_params[3];\n",
    "          if (pixel_x < 0 || pixel_x >= im_w || pixel_y < 0 || pixel_y >= im_h || cam_pt_z<0)\n",
    "              return;\n",
    "          // Skip invalid depth\n",
    "          float depth_value = depth_im[pixel_y*im_w+pixel_x];\n",
    "          if (depth_value == 0)\n",
    "              return;\n",
    "          // Integrate TSDF\n",
    "          float trunc_margin = other_params[4];\n",
    "          float depth_diff = depth_value-cam_pt_z;\n",
    "          if (depth_diff < -trunc_margin)\n",
    "              return;\n",
    "          float dist = fmin(1.0f,depth_diff/trunc_margin);\n",
    "          float w_old = weight_vol[voxel_idx];\n",
    "          float obs_weight = other_params[5];\n",
    "          float w_new = w_old + obs_weight;\n",
    "          weight_vol[voxel_idx] = w_new;\n",
    "          tsdf_vol[voxel_idx] = (tsdf_vol[voxel_idx]*w_old+obs_weight*dist)/w_new;\n",
    "          // Integrate color\n",
    "          float old_color = color_vol[voxel_idx];\n",
    "          float old_b = floorf(old_color/(256*256));\n",
    "          float old_g = floorf((old_color-old_b*256*256)/256);\n",
    "          float old_r = old_color-old_b*256*256-old_g*256;\n",
    "          float new_color = color_im[pixel_y*im_w+pixel_x];\n",
    "          float new_b = floorf(new_color/(256*256));\n",
    "          float new_g = floorf((new_color-new_b*256*256)/256);\n",
    "          float new_r = new_color-new_b*256*256-new_g*256;\n",
    "          new_b = fmin(roundf((old_b*w_old+obs_weight*new_b)/w_new),255.0f);\n",
    "          new_g = fmin(roundf((old_g*w_old+obs_weight*new_g)/w_new),255.0f);\n",
    "          new_r = fmin(roundf((old_r*w_old+obs_weight*new_r)/w_new),255.0f);\n",
    "          color_vol[voxel_idx] = new_b*256*256+new_g*256+new_r;\n",
    "        }\"\"\"\n",
    "            )\n",
    "\n",
    "            self._cuda_integrate = self._cuda_src_mod.get_function(\"integrate\")\n",
    "\n",
    "            # Determine block/grid size on GPU\n",
    "            gpu_dev = cuda.Device(0)\n",
    "            self._max_gpu_threads_per_block = gpu_dev.MAX_THREADS_PER_BLOCK\n",
    "            n_blocks = int(\n",
    "                np.ceil(\n",
    "                    float(np.prod(self._vol_dim))\n",
    "                    / float(self._max_gpu_threads_per_block)\n",
    "                )\n",
    "            )\n",
    "            grid_dim_x = min(gpu_dev.MAX_GRID_DIM_X, int(np.floor(np.cbrt(n_blocks))))\n",
    "            grid_dim_y = min(\n",
    "                gpu_dev.MAX_GRID_DIM_Y, int(np.floor(np.sqrt(n_blocks / grid_dim_x)))\n",
    "            )\n",
    "            grid_dim_z = min(\n",
    "                gpu_dev.MAX_GRID_DIM_Z,\n",
    "                int(np.ceil(float(n_blocks) / float(grid_dim_x * grid_dim_y))),\n",
    "            )\n",
    "            self._max_gpu_grid_dim = np.array(\n",
    "                [grid_dim_x, grid_dim_y, grid_dim_z]\n",
    "            ).astype(int)\n",
    "            self._n_gpu_loops = int(\n",
    "                np.ceil(\n",
    "                    float(np.prod(self._vol_dim))\n",
    "                    / float(\n",
    "                        np.prod(self._max_gpu_grid_dim)\n",
    "                        * self._max_gpu_threads_per_block\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # Get voxel grid coordinates\n",
    "            xv, yv, zv = np.meshgrid(\n",
    "                range(self._vol_dim[0]),\n",
    "                range(self._vol_dim[1]),\n",
    "                range(self._vol_dim[2]),\n",
    "                indexing=\"ij\",\n",
    "            )\n",
    "            self.vox_coords = (\n",
    "                np.concatenate(\n",
    "                    [xv.reshape(1, -1), yv.reshape(1, -1), zv.reshape(1, -1)], axis=0\n",
    "                )\n",
    "                .astype(int)\n",
    "                .T\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    @njit(parallel=True)\n",
    "    def vox2world(vol_origin, vox_coords, vox_size, offsets=(0.5, 0.5, 0.5)):\n",
    "        \"\"\"Convert voxel grid coordinates to world coordinates.\"\"\"\n",
    "        vol_origin = vol_origin.astype(np.float32)\n",
    "        vox_coords = vox_coords.astype(np.float32)\n",
    "        #    print(np.min(vox_coords))\n",
    "        cam_pts = np.empty_like(vox_coords, dtype=np.float32)\n",
    "\n",
    "        for i in prange(vox_coords.shape[0]):\n",
    "            for j in range(3):\n",
    "                cam_pts[i, j] = (\n",
    "                    vol_origin[j]\n",
    "                    + (vox_size * vox_coords[i, j])\n",
    "                    + vox_size * offsets[j]\n",
    "                )\n",
    "        return cam_pts\n",
    "\n",
    "    @staticmethod\n",
    "    @njit(parallel=True)\n",
    "    def cam2pix(cam_pts, intr):\n",
    "        \"\"\"Convert camera coordinates to pixel coordinates.\"\"\"\n",
    "        intr = intr.astype(np.float32)\n",
    "        fx, fy = intr[0, 0], intr[1, 1]\n",
    "        cx, cy = intr[0, 2], intr[1, 2]\n",
    "        pix = np.empty((cam_pts.shape[0], 2), dtype=np.int64)\n",
    "        for i in prange(cam_pts.shape[0]):\n",
    "            pix[i, 0] = int(np.round((cam_pts[i, 0] * fx / cam_pts[i, 2]) + cx))\n",
    "            pix[i, 1] = int(np.round((cam_pts[i, 1] * fy / cam_pts[i, 2]) + cy))\n",
    "        return pix\n",
    "\n",
    "    @staticmethod\n",
    "    @njit(parallel=True)\n",
    "    def integrate_tsdf(tsdf_vol, dist, w_old, obs_weight):\n",
    "        \"\"\"Integrate the TSDF volume.\"\"\"\n",
    "        tsdf_vol_int = np.empty_like(tsdf_vol, dtype=np.float32)\n",
    "        # print(tsdf_vol.shape)\n",
    "        w_new = np.empty_like(w_old, dtype=np.float32)\n",
    "        for i in prange(len(tsdf_vol)):\n",
    "            w_new[i] = w_old[i] + obs_weight\n",
    "            tsdf_vol_int[i] = (w_old[i] * tsdf_vol[i] + obs_weight * dist[i]) / w_new[i]\n",
    "        return tsdf_vol_int, w_new\n",
    "\n",
    "    def integrate(self, color_im, depth_im, cam_intr, cam_pose, obs_weight=1.0):\n",
    "        \"\"\"Integrate an RGB-D frame into the TSDF volume.\n",
    "        Args:\n",
    "          color_im (ndarray): An RGB image of shape (H, W, 3).\n",
    "          depth_im (ndarray): A depth image of shape (H, W).\n",
    "          cam_intr (ndarray): The camera intrinsics matrix of shape (3, 3).\n",
    "          cam_pose (ndarray): The camera pose (i.e. extrinsics) of shape (4, 4).\n",
    "          obs_weight (float): The weight to assign for the current observation. A higher\n",
    "            value\n",
    "        \"\"\"\n",
    "        im_h, im_w = depth_im.shape\n",
    "\n",
    "        # Fold RGB color image into a single channel image\n",
    "        color_im = color_im.astype(np.float32)\n",
    "        color_im = np.floor(\n",
    "            color_im[..., 2] * self._color_const\n",
    "            + color_im[..., 1] * 256\n",
    "            + color_im[..., 0]\n",
    "        )\n",
    "\n",
    "        if self.gpu_mode:  # GPU mode: integrate voxel volume (calls CUDA kernel)\n",
    "            for gpu_loop_idx in range(self._n_gpu_loops):\n",
    "                self._cuda_integrate(\n",
    "                    self._tsdf_vol_gpu,\n",
    "                    self._weight_vol_gpu,\n",
    "                    self._color_vol_gpu,\n",
    "                    cuda.InOut(self._vol_dim.astype(np.float32)),\n",
    "                    cuda.InOut(self._vol_origin.astype(np.float32)),\n",
    "                    cuda.InOut(cam_intr.reshape(-1).astype(np.float32)),\n",
    "                    cuda.InOut(cam_pose.reshape(-1).astype(np.float32)),\n",
    "                    cuda.InOut(\n",
    "                        np.asarray(\n",
    "                            [\n",
    "                                gpu_loop_idx,\n",
    "                                self._voxel_size,\n",
    "                                im_h,\n",
    "                                im_w,\n",
    "                                self._trunc_margin,\n",
    "                                obs_weight,\n",
    "                            ],\n",
    "                            np.float32,\n",
    "                        )\n",
    "                    ),\n",
    "                    cuda.InOut(color_im.reshape(-1).astype(np.float32)),\n",
    "                    cuda.InOut(depth_im.reshape(-1).astype(np.float32)),\n",
    "                    block=(self._max_gpu_threads_per_block, 1, 1),\n",
    "                    grid=(\n",
    "                        int(self._max_gpu_grid_dim[0]),\n",
    "                        int(self._max_gpu_grid_dim[1]),\n",
    "                        int(self._max_gpu_grid_dim[2]),\n",
    "                    ),\n",
    "                )\n",
    "        else:  # CPU mode: integrate voxel volume (vectorized implementation)\n",
    "            # Convert voxel grid coordinates to pixel coordinates\n",
    "            cam_pts = self.vox2world(\n",
    "                self._vol_origin, self.vox_coords, self._voxel_size\n",
    "            )\n",
    "            cam_pts = rigid_transform(cam_pts, np.linalg.inv(cam_pose))\n",
    "            pix_z = cam_pts[:, 2]\n",
    "            pix = self.cam2pix(cam_pts, cam_intr)\n",
    "            pix_x, pix_y = pix[:, 0], pix[:, 1]\n",
    "\n",
    "            # Eliminate pixels outside view frustum\n",
    "            valid_pix = np.logical_and(\n",
    "                pix_x >= 0,\n",
    "                np.logical_and(\n",
    "                    pix_x < im_w,\n",
    "                    np.logical_and(pix_y >= 0, np.logical_and(pix_y < im_h, pix_z > 0)),\n",
    "                ),\n",
    "            )\n",
    "            depth_val = np.zeros(pix_x.shape)\n",
    "            depth_val[valid_pix] = depth_im[pix_y[valid_pix], pix_x[valid_pix]]\n",
    "\n",
    "            # Integrate TSDF\n",
    "            depth_diff = depth_val - pix_z\n",
    "\n",
    "            valid_pts = np.logical_and(depth_val > 0, depth_diff >= -10)\n",
    "            dist = depth_diff\n",
    "\n",
    "            valid_vox_x = self.vox_coords[valid_pts, 0]\n",
    "            valid_vox_y = self.vox_coords[valid_pts, 1]\n",
    "            valid_vox_z = self.vox_coords[valid_pts, 2]\n",
    "            w_old = self._weight_vol_cpu[valid_vox_x, valid_vox_y, valid_vox_z]\n",
    "            tsdf_vals = self._tsdf_vol_cpu[valid_vox_x, valid_vox_y, valid_vox_z]\n",
    "            valid_dist = dist[valid_pts]\n",
    "            tsdf_vol_new, w_new = self.integrate_tsdf(\n",
    "                tsdf_vals, valid_dist, w_old, obs_weight\n",
    "            )\n",
    "            self._weight_vol_cpu[valid_vox_x, valid_vox_y, valid_vox_z] = w_new\n",
    "            self._tsdf_vol_cpu[valid_vox_x, valid_vox_y, valid_vox_z] = tsdf_vol_new\n",
    "\n",
    "            # Integrate color\n",
    "            old_color = self._color_vol_cpu[valid_vox_x, valid_vox_y, valid_vox_z]\n",
    "            old_b = np.floor(old_color / self._color_const)\n",
    "            old_g = np.floor((old_color - old_b * self._color_const) / 256)\n",
    "            old_r = old_color - old_b * self._color_const - old_g * 256\n",
    "            new_color = color_im[pix_y[valid_pts], pix_x[valid_pts]]\n",
    "            new_b = np.floor(new_color / self._color_const)\n",
    "            new_g = np.floor((new_color - new_b * self._color_const) / 256)\n",
    "            new_r = new_color - new_b * self._color_const - new_g * 256\n",
    "            new_b = np.minimum(\n",
    "                255.0, np.round((w_old * old_b + obs_weight * new_b) / w_new)\n",
    "            )\n",
    "            new_g = np.minimum(\n",
    "                255.0, np.round((w_old * old_g + obs_weight * new_g) / w_new)\n",
    "            )\n",
    "            new_r = np.minimum(\n",
    "                255.0, np.round((w_old * old_r + obs_weight * new_r) / w_new)\n",
    "            )\n",
    "            self._color_vol_cpu[valid_vox_x, valid_vox_y, valid_vox_z] = (\n",
    "                new_b * self._color_const + new_g * 256 + new_r\n",
    "            )\n",
    "\n",
    "    def get_volume(self):\n",
    "        if self.gpu_mode:\n",
    "            cuda.memcpy_dtoh(self._tsdf_vol_cpu, self._tsdf_vol_gpu)\n",
    "            cuda.memcpy_dtoh(self._color_vol_cpu, self._color_vol_gpu)\n",
    "        return self._tsdf_vol_cpu, self._color_vol_cpu\n",
    "\n",
    "    def get_point_cloud(self):\n",
    "        \"\"\"Extract a point cloud from the voxel volume.\"\"\"\n",
    "        tsdf_vol, color_vol = self.get_volume()\n",
    "\n",
    "        # Marching cubes\n",
    "        verts = measure.marching_cubes_lewiner(tsdf_vol, level=0)[0]\n",
    "        verts_ind = np.round(verts).astype(int)\n",
    "        verts = verts * self._voxel_size + self._vol_origin\n",
    "\n",
    "        # Get vertex colors\n",
    "        rgb_vals = color_vol[verts_ind[:, 0], verts_ind[:, 1], verts_ind[:, 2]]\n",
    "        colors_b = np.floor(rgb_vals / self._color_const)\n",
    "        colors_g = np.floor((rgb_vals - colors_b * self._color_const) / 256)\n",
    "        colors_r = rgb_vals - colors_b * self._color_const - colors_g * 256\n",
    "        colors = np.floor(np.asarray([colors_r, colors_g, colors_b])).T\n",
    "        colors = colors.astype(np.uint8)\n",
    "\n",
    "        pc = np.hstack([verts, colors])\n",
    "        return pc\n",
    "\n",
    "    def get_mesh(self):\n",
    "        \"\"\"Compute a mesh from the voxel volume using marching cubes.\"\"\"\n",
    "        tsdf_vol, color_vol = self.get_volume()\n",
    "\n",
    "        # Marching cubes\n",
    "        verts, faces, norms, vals = measure.marching_cubes_lewiner(tsdf_vol, level=0)\n",
    "        verts_ind = np.round(verts).astype(int)\n",
    "        verts = (\n",
    "            verts * self._voxel_size + self._vol_origin\n",
    "        )  # voxel grid coordinates to world coordinates\n",
    "\n",
    "        # Get vertex colors\n",
    "        rgb_vals = color_vol[verts_ind[:, 0], verts_ind[:, 1], verts_ind[:, 2]]\n",
    "        colors_b = np.floor(rgb_vals / self._color_const)\n",
    "        colors_g = np.floor((rgb_vals - colors_b * self._color_const) / 256)\n",
    "        colors_r = rgb_vals - colors_b * self._color_const - colors_g * 256\n",
    "        colors = np.floor(np.asarray([colors_r, colors_g, colors_b])).T\n",
    "        colors = colors.astype(np.uint8)\n",
    "        return verts, faces, norms, colors\n",
    "\n",
    "\n",
    "def rigid_transform(xyz, transform):\n",
    "    \"\"\"Applies a rigid transform to an (N, 3) pointcloud.\"\"\"\n",
    "    xyz_h = np.hstack([xyz, np.ones((len(xyz), 1), dtype=np.float32)])\n",
    "    xyz_t_h = np.dot(transform, xyz_h.T).T\n",
    "    return xyz_t_h[:, :3]\n",
    "\n",
    "\n",
    "def get_view_frustum(depth_im, cam_intr, cam_pose):\n",
    "    \"\"\"Get corners of 3D camera view frustum of depth image\"\"\"\n",
    "    im_h = depth_im.shape[0]\n",
    "    im_w = depth_im.shape[1]\n",
    "    max_depth = np.max(depth_im)\n",
    "    view_frust_pts = np.array(\n",
    "        [\n",
    "            (np.array([0, 0, 0, im_w, im_w]) - cam_intr[0, 2])\n",
    "            * np.array([0, max_depth, max_depth, max_depth, max_depth])\n",
    "            / cam_intr[0, 0],\n",
    "            (np.array([0, 0, im_h, 0, im_h]) - cam_intr[1, 2])\n",
    "            * np.array([0, max_depth, max_depth, max_depth, max_depth])\n",
    "            / cam_intr[1, 1],\n",
    "            np.array([0, max_depth, max_depth, max_depth, max_depth]),\n",
    "        ]\n",
    "    )\n",
    "    view_frust_pts = rigid_transform(view_frust_pts.T, cam_pose).T\n",
    "    return view_frust_pts\n",
    "\n",
    "\n",
    "def meshwrite(filename, verts, faces, norms, colors):\n",
    "    \"\"\"Save a 3D mesh to a polygon .ply file.\"\"\"\n",
    "    # Write header\n",
    "    ply_file = open(filename, \"w\")\n",
    "    ply_file.write(\"ply\\n\")\n",
    "    ply_file.write(\"format ascii 1.0\\n\")\n",
    "    ply_file.write(\"element vertex %d\\n\" % (verts.shape[0]))\n",
    "    ply_file.write(\"property float x\\n\")\n",
    "    ply_file.write(\"property float y\\n\")\n",
    "    ply_file.write(\"property float z\\n\")\n",
    "    ply_file.write(\"property float nx\\n\")\n",
    "    ply_file.write(\"property float ny\\n\")\n",
    "    ply_file.write(\"property float nz\\n\")\n",
    "    ply_file.write(\"property uchar red\\n\")\n",
    "    ply_file.write(\"property uchar green\\n\")\n",
    "    ply_file.write(\"property uchar blue\\n\")\n",
    "    ply_file.write(\"element face %d\\n\" % (faces.shape[0]))\n",
    "    ply_file.write(\"property list uchar int vertex_index\\n\")\n",
    "    ply_file.write(\"end_header\\n\")\n",
    "\n",
    "    # Write vertex list\n",
    "    for i in range(verts.shape[0]):\n",
    "        ply_file.write(\n",
    "            \"%f %f %f %f %f %f %d %d %d\\n\"\n",
    "            % (\n",
    "                verts[i, 0],\n",
    "                verts[i, 1],\n",
    "                verts[i, 2],\n",
    "                norms[i, 0],\n",
    "                norms[i, 1],\n",
    "                norms[i, 2],\n",
    "                colors[i, 0],\n",
    "                colors[i, 1],\n",
    "                colors[i, 2],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Write face list\n",
    "    for i in range(faces.shape[0]):\n",
    "        ply_file.write(\"3 %d %d %d\\n\" % (faces[i, 0], faces[i, 1], faces[i, 2]))\n",
    "\n",
    "    ply_file.close()\n",
    "\n",
    "\n",
    "def pcwrite(filename, xyzrgb):\n",
    "    \"\"\"Save a point cloud to a polygon .ply file.\"\"\"\n",
    "    xyz = xyzrgb[:, :3]\n",
    "    rgb = xyzrgb[:, 3:].astype(np.uint8)\n",
    "\n",
    "    # Write header\n",
    "    ply_file = open(filename, \"w\")\n",
    "    ply_file.write(\"ply\\n\")\n",
    "    ply_file.write(\"format ascii 1.0\\n\")\n",
    "    ply_file.write(\"element vertex %d\\n\" % (xyz.shape[0]))\n",
    "    ply_file.write(\"property float x\\n\")\n",
    "    ply_file.write(\"property float y\\n\")\n",
    "    ply_file.write(\"property float z\\n\")\n",
    "    ply_file.write(\"property uchar red\\n\")\n",
    "    ply_file.write(\"property uchar green\\n\")\n",
    "    ply_file.write(\"property uchar blue\\n\")\n",
    "    ply_file.write(\"end_header\\n\")\n",
    "\n",
    "    # Write vertex list\n",
    "    for i in range(xyz.shape[0]):\n",
    "        ply_file.write(\n",
    "            \"%f %f %f %d %d %d\\n\"\n",
    "            % (\n",
    "                xyz[i, 0],\n",
    "                xyz[i, 1],\n",
    "                xyz[i, 2],\n",
    "                rgb[i, 0],\n",
    "                rgb[i, 1],\n",
    "                rgb[i, 2],\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5528c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding Base Dir Path /monoscene/MonoScene\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "BASE_DIR = os.getcwd().rsplit(\"/\",1)[0]\n",
    "sys.path.append(BASE_DIR)\n",
    "print(\"adding Base Dir Path\", BASE_DIR)\n",
    "import numpy as np\n",
    "import torch\n",
    "# import fusion\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def read_calib(calib_path):\n",
    "        \"\"\"\n",
    "        Modify from https://github.com/utiasSTARS/pykitti/blob/d3e1bb81676e831886726cc5ed79ce1f049aef2c/pykitti/utils.py#L68\n",
    "        :param calib_path: Path to a calibration text file.\n",
    "        :return: dict with calibration matrices.\n",
    "        \"\"\"\n",
    "        calib_all = {}\n",
    "        with open(calib_path, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                if line == \"\\n\":\n",
    "                    break\n",
    "                key, value = line.split(\":\", 1)\n",
    "                calib_all[key] = np.array([float(x) for x in value.split()])\n",
    "\n",
    "        # reshape matrices\n",
    "        calib_out = {}\n",
    "        # 3x4 projection matrix for left camera\n",
    "        calib_out[\"P2\"] = calib_all[\"P2\"].reshape(3, 4)\n",
    "        calib_out[\"Tr\"] = np.identity(4)  # 4x4 matrix\n",
    "        calib_out[\"Tr\"][:3, :4] = calib_all[\"Tr\"].reshape(3, 4)\n",
    "        return calib_out\n",
    "\n",
    "\n",
    "def vox2pix(cam_E, cam_k, \n",
    "            vox_origin, voxel_size, \n",
    "            img_W, img_H, \n",
    "            scene_size):\n",
    "    \"\"\"\n",
    "    compute the 2D projection of voxels centroids\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    cam_E: 4x4\n",
    "       =camera pose in case of NYUv2 dataset\n",
    "       =Transformation from camera to lidar coordinate in case of SemKITTI\n",
    "    cam_k: 3x3\n",
    "        camera intrinsics\n",
    "    vox_origin: (3,)\n",
    "        world(NYU)/lidar(SemKITTI) cooridnates of the voxel at index (0, 0, 0)\n",
    "    img_W: int\n",
    "        image width\n",
    "    img_H: int\n",
    "        image height\n",
    "    scene_size: (3,)\n",
    "        scene size in meter: (51.2, 51.2, 6.4) for SemKITTI and (4.8, 4.8, 2.88) for NYUv2\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    projected_pix: (N, 2)\n",
    "        Projected 2D positions of voxels\n",
    "    fov_mask: (N,)\n",
    "        Voxels mask indice voxels inside image's FOV \n",
    "    pix_z: (N,)\n",
    "        Voxels'distance to the sensor in meter\n",
    "    \"\"\"\n",
    "    # Compute the x, y, z bounding of the scene in meter\n",
    "    vol_bnds = np.zeros((3,2))\n",
    "    vol_bnds[:,0] = vox_origin\n",
    "    vol_bnds[:,1] = vox_origin + np.array(scene_size)\n",
    "\n",
    "    # Compute the voxels centroids in lidar cooridnates\n",
    "    vol_dim = np.ceil((vol_bnds[:,1]- vol_bnds[:,0])/ voxel_size).copy(order='C').astype(int)\n",
    "    xv, yv, zv = np.meshgrid(\n",
    "            range(vol_dim[0]),\n",
    "            range(vol_dim[1]),\n",
    "            range(vol_dim[2]),\n",
    "            indexing='ij'\n",
    "          )\n",
    "    vox_coords = np.concatenate([\n",
    "            xv.reshape(1,-1),\n",
    "            yv.reshape(1,-1),\n",
    "            zv.reshape(1,-1)\n",
    "          ], axis=0).astype(int).T\n",
    "\n",
    "    # Project voxels'centroid from lidar coordinates to camera coordinates\n",
    "    cam_pts = TSDFVolume.vox2world(vox_origin, vox_coords, voxel_size)\n",
    "    cam_pts = rigid_transform(cam_pts, cam_E)\n",
    "\n",
    "    # Project camera coordinates to pixel positions\n",
    "    projected_pix = TSDFVolume.cam2pix(cam_pts, cam_k)\n",
    "    pix_x, pix_y = projected_pix[:, 0], projected_pix[:, 1]\n",
    "\n",
    "    # Eliminate pixels outside view frustum\n",
    "    pix_z = cam_pts[:, 2]\n",
    "    fov_mask = np.logical_and(pix_x >= 0,\n",
    "                np.logical_and(pix_x < img_W,\n",
    "                np.logical_and(pix_y >= 0,\n",
    "                np.logical_and(pix_y < img_H,\n",
    "                pix_z > 0))))\n",
    "\n",
    "\n",
    "    return torch.from_numpy(projected_pix), torch.from_numpy(fov_mask), torch.from_numpy(pix_z)\n",
    "\n",
    "\n",
    "\n",
    "def get_grid_coords(dims, resolution):\n",
    "    \"\"\"\n",
    "    :param dims: the dimensions of the grid [x, y, z] (i.e. [256, 256, 32])\n",
    "    :return coords_grid: is the center coords of voxels in the grid\n",
    "    \"\"\"\n",
    "\n",
    "    g_xx = np.arange(0, dims[0] + 1)\n",
    "    g_yy = np.arange(0, dims[1] + 1)\n",
    "    sensor_pose = 10\n",
    "    g_zz = np.arange(0, dims[2] + 1)\n",
    "\n",
    "    # Obtaining the grid with coords...\n",
    "    xx, yy, zz = np.meshgrid(g_xx[:-1], g_yy[:-1], g_zz[:-1])\n",
    "    coords_grid = np.array([xx.flatten(), yy.flatten(), zz.flatten()]).T\n",
    "    coords_grid = coords_grid.astype(np.float)\n",
    "\n",
    "    coords_grid = (coords_grid * resolution) + resolution / 2\n",
    "\n",
    "    temp = np.copy(coords_grid)\n",
    "    temp[:, 0] = coords_grid[:, 1]\n",
    "    temp[:, 1] = coords_grid[:, 0]\n",
    "    coords_grid = np.copy(temp)\n",
    "\n",
    "    return coords_grid\n",
    "\n",
    "def get_projections(img_W, img_H):\n",
    "    scale_3ds = [1, 2]\n",
    "    data = {}\n",
    "    for scale_3d in scale_3ds:\n",
    "        scene_size = (51.2, 51.2, 6.4)\n",
    "        vox_origin = np.array([0, -25.6, -2])\n",
    "        voxel_size = 0.2\n",
    "        \n",
    "        calib = read_calib(\"/monoscene/MonoScene/calib.txt\")    \n",
    "        cam_k = calib[\"P2\"][:3, :3]\n",
    "        T_velo_2_cam = calib[\"Tr\"]\n",
    "        \n",
    "        # compute the 3D-2D mapping\n",
    "        projected_pix, fov_mask, pix_z = vox2pix(\n",
    "            T_velo_2_cam,\n",
    "            cam_k,\n",
    "            vox_origin,\n",
    "            voxel_size * scale_3d,\n",
    "            img_W,\n",
    "            img_H,\n",
    "            scene_size,\n",
    "        )            \n",
    "\n",
    "        data[\"projected_pix_{}\".format(scale_3d)] = projected_pix\n",
    "        data[\"pix_z_{}\".format(scale_3d)] = pix_z\n",
    "        data[\"fov_mask_{}\".format(scale_3d)] = fov_mask \n",
    "    return data\n",
    "\n",
    "\n",
    "def majority_pooling(grid, k_size=2):\n",
    "    result = np.zeros(\n",
    "        (grid.shape[0] // k_size, grid.shape[1] // k_size, grid.shape[2] // k_size)\n",
    "    )\n",
    "    for xx in range(0, int(np.floor(grid.shape[0] / k_size))):\n",
    "        for yy in range(0, int(np.floor(grid.shape[1] / k_size))):\n",
    "            for zz in range(0, int(np.floor(grid.shape[2] / k_size))):\n",
    "\n",
    "                sub_m = grid[\n",
    "                    (xx * k_size) : (xx * k_size) + k_size,\n",
    "                    (yy * k_size) : (yy * k_size) + k_size,\n",
    "                    (zz * k_size) : (zz * k_size) + k_size,\n",
    "                ]\n",
    "                unique, counts = np.unique(sub_m, return_counts=True)\n",
    "                if True in ((unique != 0) & (unique != 255)):\n",
    "                    # Remove counts with 0 and 255\n",
    "                    counts = counts[((unique != 0) & (unique != 255))]\n",
    "                    unique = unique[((unique != 0) & (unique != 255))]\n",
    "                else:\n",
    "                    if True in (unique == 0):\n",
    "                        counts = counts[(unique != 255)]\n",
    "                        unique = unique[(unique != 255)]\n",
    "                value = unique[np.argmax(counts)]\n",
    "                result[xx, yy, zz] = value\n",
    "    return result\n",
    "\n",
    "\n",
    "def draw(\n",
    "    voxels,\n",
    "    # T_velo_2_cam,\n",
    "    # vox_origin,\n",
    "    fov_mask,\n",
    "    # img_size,\n",
    "    # f,\n",
    "    voxel_size=0.4,\n",
    "    # d=7,  # 7m - determine the size of the mesh representing the camera\n",
    "):\n",
    "\n",
    "    fov_mask = fov_mask.reshape(-1)\n",
    "    # Compute the voxels coordinates\n",
    "    grid_coords = get_grid_coords(\n",
    "        [voxels.shape[0], voxels.shape[1], voxels.shape[2]], voxel_size\n",
    "    )\n",
    "\n",
    "\n",
    "    # Attach the predicted class to every voxel\n",
    "    grid_coords = np.vstack([grid_coords.T, voxels.reshape(-1)]).T\n",
    "\n",
    "    # Get the voxels inside FOV\n",
    "    fov_grid_coords = grid_coords[fov_mask, :]\n",
    "\n",
    "    # Get the voxels outside FOV\n",
    "    outfov_grid_coords = grid_coords[~fov_mask, :]\n",
    "\n",
    "    # Remove empty and unknown voxels\n",
    "    fov_voxels = fov_grid_coords[\n",
    "        (fov_grid_coords[:, 3] > 0) & (fov_grid_coords[:, 3] < 255), :\n",
    "    ]\n",
    "    # print(np.unique(fov_voxels[:, 3], return_counts=True))\n",
    "    outfov_voxels = outfov_grid_coords[\n",
    "        (outfov_grid_coords[:, 3] > 0) & (outfov_grid_coords[:, 3] < 255), :\n",
    "    ]\n",
    "\n",
    "    # figure = mlab.figure(size=(1400, 1400), bgcolor=(1, 1, 1))\n",
    "    colors = np.array(\n",
    "        [\n",
    "            [0,0,0],\n",
    "            [100, 150, 245],\n",
    "            [100, 230, 245],\n",
    "            [30, 60, 150],\n",
    "            [80, 30, 180],\n",
    "            [100, 80, 250],\n",
    "            [255, 30, 30],\n",
    "            [255, 40, 200],\n",
    "            [150, 30, 90],\n",
    "            [255, 0, 255],\n",
    "            [255, 150, 255],\n",
    "            [75, 0, 75],\n",
    "            [175, 0, 75],\n",
    "            [255, 200, 0],\n",
    "            [255, 120, 50],\n",
    "            [0, 175, 0],\n",
    "            [135, 60, 0],\n",
    "            [150, 240, 80],\n",
    "            [255, 240, 150],\n",
    "            [255, 0, 0],\n",
    "        ]\n",
    "    ).astype(np.uint8)\n",
    "\n",
    "    pts_colors = [f'rgb({colors[int(i)][0]}, {colors[int(i)][1]}, {colors[int(i)][2]})' for i in fov_voxels[:, 3]]\n",
    "    out_fov_colors = [f'rgb({colors[int(i)][0]//3*2}, {colors[int(i)][1]//3*2}, {colors[int(i)][2]//3*2})' for i in outfov_voxels[:, 3]]\n",
    "    pts_colors = pts_colors + out_fov_colors\n",
    "    \n",
    "    fov_voxels = np.concatenate([fov_voxels, outfov_voxels], axis=0)\n",
    "    x = fov_voxels[:, 0].flatten()\n",
    "    y = fov_voxels[:, 1].flatten()\n",
    "    z = fov_voxels[:, 2].flatten()\n",
    "    # label = fov_voxels[:, 3].flatten()\n",
    "    fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,mode='markers',\n",
    "                    marker=dict(\n",
    "                            size=2,\n",
    "                            color=pts_colors,                # set color to an array/list of desired values\n",
    "                            # colorscale='Viridis',   # choose a colorscale\n",
    "                            opacity=1.0,\n",
    "                            symbol='square'\n",
    "                        ))])\n",
    "    fig.update_layout(\n",
    "    scene = dict(\n",
    "        aspectmode='data',\n",
    "        xaxis = dict(\n",
    "            backgroundcolor=\"rgb(255, 255, 255)\",\n",
    "            gridcolor=\"black\",\n",
    "            showbackground=True,\n",
    "            zerolinecolor=\"black\",\n",
    "            nticks=4, \n",
    "            visible=False,\n",
    "            range=[-1,55],),\n",
    "        yaxis = dict(\n",
    "            backgroundcolor=\"rgb(255, 255, 255)\",\n",
    "            gridcolor=\"black\",\n",
    "            showbackground=True,\n",
    "            zerolinecolor=\"black\",\n",
    "            visible=False,\n",
    "            nticks=4, range=[-1,55],),\n",
    "        zaxis = dict(\n",
    "            backgroundcolor=\"rgb(255, 255, 255)\",\n",
    "            gridcolor=\"black\",\n",
    "            showbackground=True,\n",
    "            zerolinecolor=\"black\",\n",
    "            visible=False,\n",
    "            nticks=4, range=[-1,7],),\n",
    "        bgcolor=\"black\",\n",
    "    ),\n",
    "        \n",
    "    )\n",
    "\n",
    "    # fig = px.scatter_3d(\n",
    "    #     fov_voxels, \n",
    "    #     x=fov_voxels[:, 0], y=\"y\", z=\"z\", color=\"label\")\n",
    "    # Draw occupied inside FOV voxels\n",
    "    # plt_plot_fov = mlab.points3d(\n",
    "    #     fov_voxels[:, 0],\n",
    "    #     fov_voxels[:, 1],\n",
    "    #     fov_voxels[:, 2],\n",
    "    #     fov_voxels[:, 3],\n",
    "    #     colormap=\"viridis\",\n",
    "    #     scale_factor=voxel_size - 0.05 * voxel_size,\n",
    "    #     mode=\"cube\",\n",
    "    #     opacity=1.0,\n",
    "    #     vmin=1,\n",
    "    #     vmax=19,\n",
    "    # )\n",
    "\n",
    "    # # Draw occupied outside FOV voxels\n",
    "    # plt_plot_outfov = mlab.points3d(\n",
    "    #     outfov_voxels[:, 0],\n",
    "    #     outfov_voxels[:, 1],\n",
    "    #     outfov_voxels[:, 2],\n",
    "    #     outfov_voxels[:, 3],\n",
    "    #     colormap=\"viridis\",\n",
    "    #     scale_factor=voxel_size - 0.05 * voxel_size,\n",
    "    #     mode=\"cube\",\n",
    "    #     opacity=1.0,\n",
    "    #     vmin=1,\n",
    "    #     vmax=19,\n",
    "    # )\n",
    "\n",
    "    \n",
    "\n",
    "    # plt_plot_fov.glyph.scale_mode = \"scale_by_vector\"\n",
    "    # plt_plot_outfov.glyph.scale_mode = \"scale_by_vector\"\n",
    "\n",
    "    # plt_plot_fov.module_manager.scalar_lut_manager.lut.table = colors\n",
    "\n",
    "    # outfov_colors = colors\n",
    "    # outfov_colors[:, :3] = outfov_colors[:, :3] // 3 * 2\n",
    "    # plt_plot_outfov.module_manager.scalar_lut_manager.lut.table = outfov_colors\n",
    "\n",
    "    # mlab.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "577e0c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_relations 4\n",
      "Loading base model ()..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Removing last two layers (global_pool & classifier).\n",
      "Building Encoder-Decoder model..Done.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown resampling filter (370). Use Image.Resampling.NEAREST (0), Image.Resampling.LANCZOS (1), Image.Resampling.BILINEAR (2), Image.Resampling.BICUBIC (3), Image.Resampling.BOX (4) or Image.Resampling.HAMMING (5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1560/1806777518.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1560/1806777518.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1220\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m370\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/monoscene/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2069\u001b[0m             ]\n\u001b[1;32m   2070\u001b[0m             raise ValueError(\n\u001b[0;32m-> 2071\u001b[0;31m                 \u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Use \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" or \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2072\u001b[0m             )\n\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown resampling filter (370). Use Image.Resampling.NEAREST (0), Image.Resampling.LANCZOS (1), Image.Resampling.BILINEAR (2), Image.Resampling.BICUBIC (3), Image.Resampling.BOX (4) or Image.Resampling.HAMMING (5)"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "# from helpers import *\n",
    "import sys\n",
    "import csv\n",
    "from monoscene.models.monoscene import MonoScene\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# pipeline = pipeline(model=\"anhquancao/monoscene_kitti\")\n",
    "# model = AutoModel.from_pretrained(\n",
    "#     \"anhquancao/monoscene_kitti\", trust_remote_code=True, revision='bf033f87c2a86b60903ab811b790a1532c1ae313'\n",
    "# )#.cuda()\n",
    "model = MonoScene.load_from_checkpoint(\n",
    "        \"/monoscene/MonoScene/trained_models/monoscene_kitti.ckpt\",\n",
    "        dataset=\"kitti\",\n",
    "        n_classes=20,\n",
    "        feature = 64,\n",
    "        project_scale = 2,\n",
    "        full_scene_size = (256, 256, 32),\n",
    "    )\n",
    "\n",
    "# model = MonoScene.load_from_checkpoint(\n",
    "#         model_path,\n",
    "#         dataset=\"NYU\",\n",
    "#         feature=feature,\n",
    "#         project_scale=project_scale,\n",
    "#         fp_loss=config.fp_loss,\n",
    "#         full_scene_size=full_scene_size,\n",
    "#     )\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "img_W, img_H = 1220, 370\n",
    "\n",
    "\n",
    "def predict(img):\n",
    "    img = img.resize(1220, 370)\n",
    "    img = np.array(img, dtype=np.float32, copy=False) / 255.0\n",
    "\n",
    "    normalize_rgb = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    img = normalize_rgb(img)\n",
    "   \n",
    "    batch = get_projections(img_W, img_H)\n",
    "    batch[\"img\"] = img\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].unsqueeze(0).cuda()\n",
    "\n",
    "    pred = model(batch)\n",
    "    print(pred.keys())\n",
    "    pred = pred['ssc_logit'].squeeze()\n",
    "    \n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    pred = majority_pooling(pred, k_size=2)\n",
    "    fig = draw(pred, batch['fov_mask_2'].cpu())\n",
    "\n",
    "\n",
    "    return fig\n",
    "   \n",
    "color_image_path = \"/monoscene/MonoScene/nyu/NYU_dataset/depthbin/NYUtest/NYU0001_0000_color.jpg\"\n",
    "from PIL import Image\n",
    "image = Image.open(color_image_path)\n",
    "predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c1dde45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1560/2686397780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monoscene",
   "language": "python",
   "name": "monoscene"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
